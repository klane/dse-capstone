{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing values according to horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 12\n",
    "\n",
    "# d2020 = pickle.load(open('bay_2020.pkl','rb'))\n",
    "\n",
    "# rolling_data = d2020.rolling(horizon,min_periods=1).mean()\n",
    "# rolling_data.fillna(rolling_data.mean(),inplace = True)\n",
    "# d2020.fillna(rolling_data[d2020.isnull()],inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class that builds DCRNN data with NetworkX graph as an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "class create_dcrnn_data():\n",
    "    '''\n",
    "    This class takes a networkx graph with temporal values modeled into them and generates the adjacency matrix\n",
    "    and adjacency matrix required by the DCRNN model.\n",
    "    \n",
    "    Each node is expected to have a series with index as datetime.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,graph,**kwargs):\n",
    "        self.graph = graph\n",
    "        self.outdir = kwargs.get('outdir','')\n",
    "        self.horizon = kwargs.get('horizon',12)\n",
    "        self.df = kwargs.get('data',None)\n",
    "        \n",
    "    def _createDataFrame(self):\n",
    "        frame = list()\n",
    "        for i in range(self.graph.number_of_nodes()):\n",
    "            frame.append(self.graph.nodes[i]['values'])\n",
    "        return pd.concat(frame,axis = 1).sort_index(inplace = False)\n",
    "    \n",
    "    def generateAdjacencyMatrix(self):\n",
    "        nodes = [self.graph.nodes[i]['sensor'] for i in self.graph.nodes]\n",
    "        sensor_ind = {self.graph.nodes[i]['sensor']:i for i in self.graph.nodes}\n",
    "        matrix = [nodes,sensor_ind,networkx.adjacency_matrix(self.graph)]\n",
    "        try:\n",
    "            pickle.dump(matrix,open(os.path.join(self.outdir,'newest_adj_max.pkl'),'wb'))\n",
    "        except Error as e:\n",
    "            print(e)\n",
    "\n",
    "    def generate_graph_seq2seq_io_data(self,df, x_offsets, y_offsets, add_time_in_day=False, add_day_in_week=False, scaler=None):\n",
    "        \"\"\"\n",
    "        Return:\n",
    "        # x: (epoch_size, input_length, num_nodes, input_dim)\n",
    "        # y: (epoch_size, output_length, num_nodes, output_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        num_samples, num_nodes = df.shape\n",
    "        data = np.expand_dims(df.values, axis=-1)\n",
    "        data_list = [data]\n",
    "        \n",
    "        if add_time_in_day:\n",
    "            time_ind = (df.index.values - df.index.values.astype(\"datetime64[D]\")) / np.timedelta64(1, \"D\")\n",
    "            time_in_day = np.tile(time_ind, [1, num_nodes, 1]).transpose((2, 1, 0))\n",
    "            data_list.append(time_in_day)\n",
    "        if add_day_in_week:\n",
    "            day_in_week = np.zeros(shape=(num_samples, num_nodes, 7))\n",
    "            day_in_week[np.arange(num_samples), :, df.index.dayofweek] = 1\n",
    "            data_list.append(day_in_week)\n",
    "\n",
    "        data = np.concatenate(data_list, axis=-1)\n",
    "        \n",
    "        # epoch_len = num_samples + min(x_offsets) - max(y_offsets)\n",
    "        x, y = [], []\n",
    "        \n",
    "        # t is the index of the last observation.\n",
    "        min_t = abs(min(x_offsets))\n",
    "        max_t = abs(num_samples - abs(max(y_offsets)))  # Exclusive\n",
    "        \n",
    "        for t in range(min_t, max_t):\n",
    "            x_t = data[t + x_offsets, ...]\n",
    "            y_t = data[t + y_offsets, ...]\n",
    "            x.append(x_t)\n",
    "            y.append(y_t)\n",
    "        x = np.stack(x, axis=0)\n",
    "        y = np.stack(y, axis=0)\n",
    "        return x, y\n",
    "\n",
    "    \n",
    "    def generate_train_val_test(self):\n",
    "        if not isinstance(self.df,pd.DataFrame):\n",
    "            df = self._createDataFrame()\n",
    "        else:\n",
    "            df = self.df\n",
    "        # Make sure to test what happens at horizon = 0\n",
    "        x_offsets = np.sort(\n",
    "            np.concatenate((np.arange(1 + (-1 * self.horizon), 1, 1),))\n",
    "        )\n",
    "        \n",
    "        # Predict the next one hour\n",
    "        y_offsets = np.sort(np.arange(1, self.horizon + 1, 1))\n",
    "\n",
    "        # x: (num_samples, input_length, num_nodes, input_dim)\n",
    "        # y: (num_samples, output_length, num_nodes, output_dim)\n",
    "        x, y = self.generate_graph_seq2seq_io_data(\n",
    "            df,\n",
    "            x_offsets=x_offsets,\n",
    "            y_offsets=y_offsets,\n",
    "            add_time_in_day=True, # Add flag for this\n",
    "            add_day_in_week=False,\n",
    "        )\n",
    "\n",
    "        print(\"x shape: \", x.shape, \", y shape: \", y.shape)\n",
    "\n",
    "        num_samples = x.shape[0]\n",
    "        num_test = round(num_samples * 0.2)\n",
    "        num_train = round(num_samples * 0.7)\n",
    "        num_val = num_samples - num_test - num_train\n",
    "\n",
    "\n",
    "        x_train, y_train = x[:num_train], y[:num_train]\n",
    "        \n",
    "        x_val, y_val = (\n",
    "            x[num_train: num_train + num_val],\n",
    "            y[num_train: num_train + num_val],\n",
    "        )\n",
    "\n",
    "        x_test, y_test = x[-num_test:], y[-num_test:]\n",
    "\n",
    "        datasets = {}\n",
    "        for cat in [\"train\", \"val\", \"test\"]:\n",
    "            _x, _y = locals()[\"x_\" + cat], locals()[\"y_\" + cat]\n",
    "            print(cat, \"x: \", _x.shape, \"y:\", _y.shape)\n",
    "            np.savez_compressed(\n",
    "                os.path.join(self.outdir, \"%s.npz\" % cat),\n",
    "                x=_x,\n",
    "                y=_y,\n",
    "                x_offsets=x_offsets.reshape(list(x_offsets.shape) + [1]),\n",
    "                y_offsets=y_offsets.reshape(list(y_offsets.shape) + [1]),\n",
    "            )\n",
    "            datasets[\"x_\" + cat] = _x\n",
    "            datasets[\"y_\" + cat] = _y\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and create graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = pickle.load(open('full_graph.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sub graph (Masked Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Replace with your masking logic\n",
    "'''\n",
    "nodes = list(range(20))\n",
    "\n",
    "subgraph = g.subgraph(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flush out Input data for DCRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape:  (52090, 12, 20, 2) , y shape:  (52090, 12, 20, 2)\n",
      "train x:  (36463, 12, 20, 2) y: (36463, 12, 20, 2)\n",
      "val x:  (5209, 12, 20, 2) y: (5209, 12, 20, 2)\n",
      "test x:  (10418, 12, 20, 2) y: (10418, 12, 20, 2)\n"
     ]
    }
   ],
   "source": [
    "outdir = ''\n",
    "\n",
    "c = create_dcrnn_data(subgraph,outdir = '',horizon = horizon)\n",
    "\n",
    "c.generate_train_val_test()\n",
    "c.generateAdjacencyMatrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now train DCRNN using this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
